




\section{Discussion}

    We conclude by reflecting on the enablers and barriers to positive \ac{CA} self-report experiences, related design implications, as well as our ethical and conceptual reflections on the design and use of \acp{CA} to support mental health and wellbeing.

    \subsection{Designing for Engagement}
    
        Engagement is a prominent goal for the design of many systems within \ac{HCI}, and has been touted as one of the potential advantages of speech-enabled systems. The present study provided many detailed qualitative insights into how we might design for engagement in the practice of self-report via \ac{CA}.
        
        Firstly, many participants demonstrated high levels of engagement (see Sections~\ref{sec:perceived_engagement}~\&~\ref{sec:perceived_experience}) which often reflected broad acceptance of the technology as well as positive perceptions of the potential of \acp{CA} to support mental health and wellbeing. As motivations for their engagement in self-report via \ac{CA}, many participants, in line with prior findings~\cite{lau2018alexa, Rafal2018Workplace}, pointed to the convenience offered by the \ac{CA}'s hands-free experience; \textit{``I love it because it’s like an interface you talk directly to. It’s super easy to use. You don’t have to open your laptop and go to a specific page. I can just go home, open the door and talk to \acl{app}, super easy''} [P1]. 

        Secondly, others described speech as a more natural form of interaction, allowing them to express their emotions freely and spontaneously compared to other means of self-report; \textit{``Speaking is much easier because you can just let the words flow and you don't have to think about it''} [P14]. Still others spoke to feeling heard -- a quality that we believe is fundamental for sustainable long-term \ac{CA}-user relationships. And yet, as barriers to their engagement, participants often mentioned technical limitations which prevented users from completing their self-reports, social factors, and privacy concerns including eavesdropping and data security. Our findings therefore reflect participants' willingness to engage with \acp{CA} for the self-report of mental health and wellbeing, but also raise questions about the readiness of \acp{CA} for real-world use. 
    
        \subsubsection{Users Are Ready to Engage. How about \acp{CA}?} % Is \ac{CA} technology ready?
        
            Technical limitations (See Section~\ref{sec:ca_limitations}) undoubtedly restrained users from utilizing the \ac{CA} to its full potential, a result that aligns with prior research findings~\cite{Rafal2018Workplace}. Interestingly many of the limitations which most challenged participants are not strictly technical in nature but due to artificial constraints imposed on the interaction design itself. This highlights the need for a future human-centered approach to \ac{CA} design and broader consideration of a wider variety of use-cases for such systems. 
            
            And yet, participants' comments also point to design choices which might be made to support engagement even given the current limitations imposed on the technology, including (i) varying the questions, (ii) probing more often, and (iii) guiding the conversation more explicitly; % Designing Better Conversations
            
            \paragraph{(i)Varying More Often}
            
                Participants recommended incorporating greater variation in questions, including voice characteristics (e.g. tone and intonation), and randomizing their presentation more often so that the questions did not feel repetitive;\textit{``No one's going to ask you the same kind of question in the same way with the same intonation every time. So having a few different questions and rotating them would be better''} [P18]. In contrast however, P6 who had been diagnosed with schizophrenia, noted that the repetitiveness of certain questions provided consistency in interaction which they found important; \textit{``What's really important when you suffer from a condition like schizophrenia is consistency \ldots And in that regard, I almost find a sort of comfort in \acl{app}. She is like an anchor, which you can use to ground yourself, because she always says the same thing''}.
            
            \paragraph{(ii) Probing Further}
            
                Participants perceived the fortnightly \ac{WHO-5} questionnaire as a useful addition of variety to the daily open-ended questions; \textit{``I was happy when those questionnaires came up - it was something different at least''} [P5], and several suggested also adding more discrete questions to the conversation design; \textit{``She could toss in some of that once in a while that wouldn't actually be bad''} [P4]. While many agreed that closed-ended questions can be more efficiently answered, P9 and P10 also interestingly noted that responding to the discrete questions according to a pre-defined scale made them feel more like study subjects; \textit{``You feel a bit more like a study subjects like okay `How do you feel from 1 to 10?', it's like `okay 1\ldots 5\ldots 3'\ldots''} [P9]. 
                
            \paragraph{(iii) Guiding More}
                
                Participants finally suggested probing their responses further to engage them in a more `natural' conversation. They stressed that the \ac{CA} does not need to understand everything but could employ strategies to support a richer conversation. P20 provided an example; \textit{``For e.g., If I say, `It was a very busy day, very overwhelming', then she could ask like, `How did you deal with that?' How could you improve?, something I’d think a bit differently about? Well, obviously it’s difficult to ask a follow up question without understanding what was said. So it’d be best if there was an \ac{AI} which could pull out some key words and follow up on the basis of those keywords''}.

                Many participants also suggested alternate conversation designs. P18 commented, for example, that when depressed, they would tend to have negative thoughts all the time, and would appreciate a conversation that encourages them to talk about something positive in their lives; \textit{``She could ask me like, `Can you tell me a positive thing that happened to you today?' I can always tell you about bad things happening in my day. It would be better if I was turned away from that a little bit''} [P18]. P15 shared similar views but argued that the conversation should delve into both positive and negative emotions, including the reasons behind those emotions. P12 and P19 likewise advocated enabling users to select the topic of the conversation, envisioning a set of topics from which users could choose, allowing users to lead the conversation and reflect more on their emotions; \textit{``If it had a certain set of questions for different topics that would that would help people reflect a lot more and having to be the one leading the story through the entire way''} [P19].
            
            \vspace{0.25cm}    
            Many participants spoke of the value of the technology as allowing them to fully express their emotions, and yet also as one of the ways in which the system was currently most limited. These changes combined may allow users to express their emotions more fully.

        \subsubsection{Provide Space for Reflection}
        
            The practice of reporting entails reflection - a point a number of participants highlighted - and the design of a self-report experience might therefore equally be construed as structuring a practice of reflection. Participants' comments for improvements in turn pertained not only to the current conversational design but also to the potential to support further reflection on their own reported data. Many participants discussed the need for a visual tool for self-reflection, arguing that it would be the easiest means of searching and examining their information.
                   
            Others however, also raised the possibility of adapting the voice interface to support reflection. P14, for example, reflected on their use of a mobile app for this purpose and mentioned that they often ignored their data shown by the app. Via verbal reflection, they stated, it would feel more like confronting a problem, which they would find more meaningful than visually examining their data; \textit{``If someone is telling me like `Hey man, I've noticed that the last couple of days you have been really stressed out, is there anything wrong?' or `What's going on?'. It's more confronting\ldots and I think that gives me way more like meaning than if I just see that in the app''}.
            
            For P15, reflecting via \ac{CA} could also provide value by granting them more scope to disagree with advice provided. Compared to the potential for confrontation with friends, family, and therapist, they felt this approach could better support behavior change; \textit{``I would be way more likely to listen to a machine because even if it tells me something like a recommendation or tells me what to do, if I don't want to do it, I'm not gonna do it''}. Participants discussed different ways in which the \ac{CA} could present data verbally for users to reflect upon. They debated whether the \ac{CA} should automatically announce the data every session or on-demand. Those who supported automated voicing of the data recommended that the information should be announced casually at the end of the session. Others cautioned that the idea of presenting the data without users' request could prove intrusive depending upon the user's mental state; \textit{``It's kind of a double edged sword, because it can be intrusive if you don't want it. If you don't want any feedback, and then \acl{app} tells you, oh it's been really bad the past three weeks. That's the last thing that you want to hear''} [P8].
            
        \subsubsection{Maintaining Focus on 'The Why'}
             
            This study therefore reveals the extent to which, when designing for engagement, it is also important to consider users', designers' and researchers' implicit and explicit motivations. One of the most striking aspects of our findings concerns the extent to which participants' appropriation of the technology into their lives and for their own purposes reflects the diverse forms of value they associated with the technology. P14 and P6 even took the smart speaker with them when away from home; \textit{``So for three weeks I was in the sort of caravan and actually it worked fine. I just had to reset it to my, to the Wi-Fi, and that was not a problem''}[P14].

            When thinking about engagement as a design goal it is therefore important to avoid a narrow focus on engagement as an aim in itself, and to think about how the technology best supports participants' own health and wellbeing goals, and how users are best able to appropriate the technology into their own lives. This is particularly important for patients living with \ac{AD}, and brings us to the ethics of designing for vulnerable populations.

    \subsection{A Design Ethics Founded in Respect for Vulnerability}
    
        Participants' involvement in this study required enabling high levels of trust given the potentially vulnerable nature of the population group, the stigma surrounding mental illness, and the novel nature of the technology itself.
    
        Participants' perceptions of \acp{app} as a good listener, a companion, and a tool for emotional venting and self-talking suggest the potential of \ac{CA}s to prove of meaningful value to users, and yet also raises questions concerning the potential ethical ramifications of \ac{CA} personification. Such connections and associations may, for example, serve as a means of overcoming stigma, or strengthening it, should that stigma become attached to the technology itself. % \ac{CA} Interventions reduced positive and negative volitional stigma but not traditional stigma (social distance)~\cite{sebastian2017changing}

        It is also important to question the extent to which these agents are truly capable of serving as companions, and indeed the extent to which we desire them to do so. Can a \ac{CA}, for example, play the role of a therapist? The therapeutic, and healthcare literature more broadly, highlights the role of relationship and rapport as mediating factors in the efficacy of care. Is there such a thing as 'the uncanny value of caring? Where does it start and where does it end? This research suggests that users may find value in connecting to agents to a greater extent than even we as developers imagined given the limited nature of this agent, and yet also underlines the need for future research if these technologies are to be implemented and deployed in ethical ways.
        
    \subsection{Is Conversational Self-Report an Oxymoron?}
    
        One of the as-of-yet unspoken questions underlining this framing of technology concerns whether self-report is best framed as a conversation at all. And indeed, where the values lies in doing so. The term 'report' implies a certain power (im)balance which does not necessarily reflect how all participants in this study conceived of their relationship to \acl{app}, when they considered this a relationship at all. The phrase 'designing for conversation' may therefore itself be read as a reframing of the very idea of self-report.

        \subsubsection{Pinning Down Our Own Expression of A Relationship-Oriented Design Framing}
        
            As these devices grow ever more ubiquitous, researchers are beginning to question, as in this work, ways of understanding and designing for increasingly meaningful and longitudinal forms of interaction — which may equally be characterized as relationships. This is not a trivial change in orientation, and one which has featured strongly in our thinking around and framing of the design of \acl{app}. We perceive value in asking the question; 'What kind of relationship is this?' What kind of relationships do we hope to embody and support via these systems, and which expectations do we intend to set for users in this respect? Indeed, even more broadly, which characteristics of human-to-human relationships translate to human-to-agent relationships? Is this for example, a professional, casual, therapeutic or wholly transactional relationship, and which design choices realize these modes not only of interacting, but relating. 
            
            Participants' descriptions of \acl{app} indeed ranged from `this round little thing' to `this little person in my life,' although leaned surprisingly often more in the direction of humanization and personification of the agent. In part participants' varied interpretations of the agent may be explained by our own open-ended presentation of the system -- a conscious choice. We often struggled to provide a concise description of our own framing of this agent, something we have noticed much nascent work in this space struggles to achieve, as researchers and designers strive to strike the appropriate balance between human and machine. To overcome such barriers to action and clarify our own expression, it can be useful to ask ourselves such questions as 'From the participant's point of view, who, or what, are they reporting to?' Who is the conversation with? And where does conversation add value? Making these kinds of conceptions clear to participants is likely key to engendering trust.

            \ac{VUI} technologies are increasingly presented as capable of supporting more human modes of interaction, and in turn characterized and analyzed in terms of interactional traits, from responsiveness to tone and ease of understanding. We argue that designers of \ac{VUI} systems must also consider how particular relationship framings interact with the limitations and possibilities of technology. A more professional framing for example, can enable designers to side-step certain limitations of the technology by, from the offset, setting expectations which preclude more casual and intimate forms of interaction. We must then ask, which technological futures do we desire?

    \subsection{Desired Futures} % What else needs to be done to improve \ac{CA} design for self-reports of mental health and wellbeing?
    
        \begin{quote}
        \vspace{2mm}
            \textit{``If \acl{app} can do, not to automate the life, to be someone to talk to. Yes, I can do that everyday. I can trust to talk to \acl{app} everyday. I don't know if you have ever seen the movie, `Her'~\cite{spike2014her}. Something like that. The `Her' thing is not to automate your life or to remind you anything. Google Assistant can do that. Someone to talk with, not to talk with to get your feedback, just to talk with to review your day and to see okay what you're actually doing.''} [P16]
        \vspace{2mm}
        \end{quote} 
    
        Looking forward, as smart-speaker devices become more mainstream, it is worth considering the futures we desire, and how we might design to sustainably support them. What role do we want these devices to play in our lives? 
        
        Although most participants expressed hesitation in considering health recommendations provided by a \ac{CA}, several mentioned that they would welcome non-intrusive recommendations that reminded them of the things they could do, although added that a \ac{CA} should allow them to make their own decisions to act;  \textit{``I mean sometimes you forget. So you can remind sometimes. It should be not like `Okay, here you have a breathing exercise'. Like `Do you want to do a breathing exercise to improve your mental health?' and then you can say `Yes' or `No'. I think it's important that you still have the possibility to say no''} [P14]. Other participants suggested incorporating humour and advice into conversations with \acl{app}.
        
        These possibilities raise additional questions. Do we, for example, want agents to provide recommendations for our health and wellbeing, or prove truly capable of holding rich conversations, or might we prefer uncaring machines, who may also ironically be best placed to provide an experience of care? These are the kinds of questions raised by this study, which is one of the first to engage a vulnerable group in the real-world use of such technologies, despite the fact that many such systems are available and continue to emerge on the commercial app stores. It is time we begin to reflect on the implications of growing adoption of these technologies, and we hope this work serves as a step in the direction of such a discourse.
                
% UNUSED NOTES
% \textbf{Our findings reveal users' experience as strongly shaped by actions taken to overcome the technological limitations of \acp{CA}, diverse personified perceptions of a self-report agent, the socially-contingent nature of self-reporting practice as well as users' reflections on privacy and security concerns.}
% \subsubsection{Emulate emotional support}
% \subsubsection{Establish positive tone or relationship}
% Privacy perception and data security concerns presented in this study show users' trust towards the governing bodies, yet their confidence over smart speaker companies remain skeptical. 
% Based on these results, we discuss: a, b, c 
% with respect to \ac{CA}'s potential in empowering users to self-report their mental health and wellbeing.
% We then, highlight several important design considerations for future design of \ac{CA} to support self-report of mental health and wellbeing.
% Notably, this result also reflects the gulf between the technology and the users expectation of being able to have a human-human-like conversation~\cite{Gulfbetwee2016ewa} despite setting a realistic expectations before \ac{CA} use. This leads 
% the question whether the technology is capable of engaging users for a desired outcome.
% participants with lower than 60\% of adherence rate (P1, P11, P20) reasoned that they were unable to use \ac{CA} due to the immobility of the smartspeaker, their own mental state and privacy reasons. 
% % immobility of the smartspeaker, 
% \textit{``When I am quite down, I close down. I don’t talk to anyone. It’s not just \acl{app}. During the times I was lived with my boyfriend, it was difficult for me to ask him to leave the room as I wanted privacy to talk to \acl{app}. And sometime, I wasn’t in town''}.
% Enablers
% 1. There are so many technical limitations of \ac{CA} but people are ready and willing to use the system - reflected by our results:
% UEQ 
% Adherence
% tactics they used and 
% the social and mental benifits they outlined.
% Enablers of Engagement
% Despite many technical challenges, results 
% Will \ac{CA} be able to relate 
% P14 in particular questioned a \ac{CA}'s ability to relate users' emotions to their past and commented as a result that \acp{CA} might not be able to establish truly effective patient-therapist (\ac{CA} or human) relationships while acknowledging that talking to \acp{CA} might help one to reflect on their emotions.
% \textit{``You can have self reflection and you can figure out things by yourself. But sometimes it is also important that someone else can say `okay, you're feeling this way but maybe it comes from this' and they can relate things that have been mentioned in other conversations or those relationships cannot be made with \acl{app} so and that's why I think it's more useful to have sometimes meetings with the psychiatrists.''} [P14]
% Reasons for Low Rates of
% Participants with lower than 60\% of adherence rate (P1, P11, P20) reasoned that they were unable to use \ac{CA} due to
% the immobility of the smartspeaker, 
% their own mental state and 
% privacy reasons. 
% immobility of the smartspeaker, 
% P1, for example, when asked about her low adherence rate, they explained: 
% \textit{``When I am quite down, I close down. I don’t talk to anyone. It’s not just \acl{app}. During the times I was lived with my boyfriend, it was difficult for me to ask him to leave the room as I wanted privacy to talk to \acl{app}. And sometime, I wasn’t in town''}.
% Despite the static nature of the smart speaker, P14 and P6 however, took the smart speaker with them when they were away from their home and described that it was not much of a hassle to set up the system in a new place as long as they had access to the electric outlet and Wi-fi.
% \begin{quote}
% \textit{``So for three weeks I was in the sort of caravan and actually it worked fine. I just had to reset it to my, to the Wi-Fi, and that was not a problem.''}
% [P14]
% r27
% \end{quote}    
% Participants shared many valuable suggestions for the future design of a \acp{CA} to support self-reports of mental health and wellbeing. Their suggestions included ways to 
% (i) improve \ac{CA}'s conversational skill;
% (ii) reflect on self-reports; and provide
% (iii) health recommendations.
% ``past is just a story we tell to ourselves''~\cite{spike2014her}
% Who are they reporting to in participant's view?
% Who is the conversation with?
% Where and how does `conversation' add nuance?
% P7 shared similar thoughts and added that appropriate probing would make the conversation more natural and emulate the sense of being heard.
%     \textit{``I would like some maybe maybe basic minimal but some sort of a simulation of listening you know, like she like she would pick up on the small things in the conversation and ask an ask questions based on that. Of course that's the hardest part you know, making them think Yeah, you know, like the way the natural people communicate you know, like if someone if someone said you know, `Man my boss really frustrates me', you know, then \acl{app} could ask him, `Oh, what is so frustrating about her?' or `What did she say?' and stuff like that. That sort of things will make the conversation more organic''}.
% ----sense of confrontation----
% ---- \ac{CA} doesn't make them feel like shit ----
% HOW to present the data for reflection?
% WHY NOT Vocal reflection?
% It is therefore important to ask the question as to whether \ac{CA} technology is ready for such use cases. 
% Suggesting a supplemental mobile app to reflect on their data, P7 said, 
% \textit{``If \acl{app} was telling me my statistics, I would probably space out through half of it. And I would be like, yeah, is there an option to repeat this? So a supplemental app would be great''}.
% Assuming that \acl{CA} would play the recordings of their self-reports to reflect on P5 noted that they did not like to listen to themselves and the graphical would be more appropriate for reflection:
% \textit{``I don’t like to listen to myself. Reading could be better. You can search easily with your eyes and go through the words - find interesting parts''}.
% Participants shared a common view of the need for a visual tool to reflect on their data although many also entertained the idea of verbal reflection. While several participants expressed their interest in verbal reflection, some also believed that it could serve as an effective tool for sustainable behavior change.
% This included (i) making multiple entries, (ii) adapting one's speech, and (iii) preparing in advance to self-report. A
% Many of participants comments also reflected their own respect for the thoughts of their social circle.
% Narratives of the participants' social circle strengthens \ac{CA}'s social acceptance and its potential in supporting users' relationship with their significant others, albeit, .
% Awareness of this position raises questions concerning the ethics of various framings of the technology. Many researches are working on personified interpretations of agents [cite work on penguins etc], and in this study many participants assigned human-like qualities to \acl{app}. And yet 
% Participants suggested several other suggestion to support engaging self-report via \ac{CA}. These design suggestions included 
% (i) notification, 
% (ii) humor, and
% (iii) health information.
% notification for adherence
% P1 and P16 suggested notifying users to talk to \ac{CA} if they have not self-reported for certain period of time.
% \textit{``When I am down, I close myself to the world, it would be an important moment for me to actually speak. Therefore, if I have not been speaking for lets say 48 hrs, \acl{app} could notify me or ask me, `Hey, are you feeling okay?' ''} [P1].
% humor
% Echoing the prior literature~\cite{clark2019makes}, P4 remarked humor in the conversation could make them feel positive:
% \textit{``If you can get people to smile maybe they'll be more positive about their day...say something funny''}.
% information on the issue	
% P4 also mentioned that providing relevant health information (e.g., symptoms of a health condition) could make feel that they are not the alone:
% \textit{``It could read you something from the web page, like `(name of the site) says this about anxiety or blah blah blah'. That makes you kind of like, Okay other people feel these things also when they have anxiety'' }.