\section{Related Work}\label{sec:related_work}

    This study builds upon and contributes to prior research concerning the study and design of \ac{CA}s for mental health and wellbeing, as well as speech-enabled self-report mechanisms.

    \subsection{\aclp{CA} for Mental Health \& Wellbeing}\label{sec:ca_mentah_health}
    
        The history of \ac{CA}s for mental healthcare dates back to the development of ELIZA in 1966. This first of many text-based \acl{NLP} programs to come provided pre-determined answers to user queries in mimicry of a Rogerian psychotherapist~\cite{eliza1966Weizenbaum}. Technology has advanced significantly in the years since with the emergence of new forms of \ac{IVR},\ac{ASR} and embodied systems, chatbots and speech-enabled \ac{CA}s. 
        
        Although \ac{CA}s remain in many ways nascent technologies, many \ac{HCI} and healthcare researchers have already begun to explore \ac{CA}s' potential to support mental health and wellbeing in a variety of contexts and in various forms, from diagnosis to symptom monitoring, treatment and intervention~\cite{gaffney2019conversational, laranjo2018conversational, vaidyam2019chatbots, lovejoy2019technology}. The multi-modal embodied system `Virtual Mindfulness Coach', for example, was  developed and evaluated for its effectiveness in training and coaching mindfulness meditation~\cite{hudlicka2013virtual}. Student users of this system were found to achieve a higher sense of self-efficacy in regard to the establishment of regular mindfulness practice through coach-based training than via self-administered training using written and audio materials. A remote `Wizard-of-Oz' study of another multi-modal agent designed to provide longitudinal social support to isolated older adults similarly reported high levels of user acceptance and satisfaction with the system~\cite{bickmore2005social}. Findings suggested that the system's capacity to address loneliness relied on agents' ability to interact proactively with users rather than waiting passively for users to initiate interaction.

        Other studies have demonstrated that human-CA interactions tend to follow prevailing social norms despite users' awareness that their interaction partner is a machine~\cite{nass1993anthropomorphism, reeves1996media}. It has been suggested that this more human-like conversational mode of interaction may enable users to form a relationship with \ac{CA}s~\cite{waytz2010sees, bickmore2005establishing}, and that \ac{CA}s could in turn potentially serve as means to collect more accurate, honest and insightful information from users~\cite{devault2014simsensei, lucas2014s}. In the broader context of the design of technology to support mental health, \ac{CA}s' potential capacity to establish relationships with users could eventually facilitate even health-related behavior change among users with \acp{AD}~\cite{thieme2015designing, bickmore2005establishing}.
        
        \subsubsection{\aclp{CA} \& \aclp{AD}}\label{sec:ca_ad}
            
            One in 4 people worldwide will be affected by an \ac{AD} in their lifetime \cite{world2001world}. This class of mental health conditions characterized by dramatic changes in mood includes depression, bipolar disorder, anxiety and other mood disorders~\cite{harrington1994affective}. Therapies entailing practices of self-report and self-monitoring such as \ac{CBT} and \ac{BA} are known to be effective in treating \ac{AD}s~\cite{cuijpers2010guided, weiss2016can,tindall2017behavioural}. The efficacious delivery of treatment however is often impacted by challenges including long waiting lists caused by a shortage of mental health professionals, stigma in relation to mental health, and a lack of access related to limited availability as well as inefficient and burdensome means of communication~\cite{inkster2018empathy}. 
            
            Initial research suggests that by aiding in the assessment and treatment of mental illnesses including \ac{AD}s, \ac{CA}s may prove able to play a role in overcoming these challenges of access and interaction ~\cite{hoermann2017application, vaidyam2019chatbots}. Fitzpatrick et al., for example, developed a fully automated chatbot named `Woebot' and studied its feasibility, acceptability, and efficacy in delivering a self-help \ac{CBT} program for college students with symptoms of anxiety and depression~\cite{fitzpatrick2017delivering}. Findings of this study showed that delivery of \ac{CBT} through the chatbot significantly reduced participants' symptoms of depression and anxiety. The authors furthermore suggest that process factors were more influential for participants' acceptance of the system than content factors, mirroring traditional therapy. 
            
            Denecke et al. conducted a usability study of `SERMO', a chatbot implementing \ac{CBT} methods for emotional regulation, with $4$ clinicians, $9$ patients and $8$ non-patients~\cite{denecke2020mental}. Participants rated the app positively in terms of efficiency, perspicuity and attractiveness, and its hedonic qualities (stimulation and novelty) neutrally. Inkster et al. evaluated the effectiveness of an \ac{AI}-enabled text-based \ac{CA} named `Wysa' for the elicitation of self-reported symptoms of depression using the \ac{PHQ-9} questionnaire as well as improvements to users' mood~\cite{inkster2018empathy}. Participants reported that the \ac{CA} proved helpful and a source of encouragement in relation to their mood. And, Bickmore et al. investigated the usability and effectiveness of a text-based virtual nurse named Elizabeth with 139 patients with depressive symptoms and found that the \ac{CA} could serve as an effective medium for automated screening and delivery of treatment~\cite{bickmore2010response}.

            While the results of these preliminary studies are promising, therapeutic applications of \ac{CA}s remain at a very early stage and there are many technical, ethical and medical challenges which must be overcome before they can be offered as a viable alternative for those in need of support~\cite{vaidyam2019chatbots}. First we must in particular develop an understanding of the perceptions and experiences of people living with \ac{AD}s in relation to the disclosure of their mental health and wellbeing by means of \ac{CA}. 
            
    \subsection{Understanding Speech-Enabled Self-Report Mechanisms}
        
        Much research in recent years has focused on the capacity of mobile devices  to support the self-report of mental health and wellbeing. \ac{GUI} based tools have in many cases enabled the efficient collection of textual and visual data of many forms, and yet there persist numerous motivations for considering alternative forms of self-report. In particular, these systems are but one medium of self-expression, often constrain users' responses, pose challenges for the assessment of the validity and reliability of data gathered, and can place a significant burden on users~\cite{harari2016using, van2017experience, doherty2020design}.
        
        This has led a number of researchers to turn towards the development of speech-enabled methods of self-report. One such research thread concerns work in the healthcare domain focused on speech-enabled embodied \ac{CA}s as a means of eliciting self-reports of psychological symptoms from patients. Lucas et al., for example, examined the impact of a virtual human interviewer on the disclosure of \ac{PTSD} symptoms among active-duty service members including the factors influencing self-reporting practices~\cite{lucas2017reporting}. Results of this study suggested that virtual human interviewers could foster increased patient disclosure of mental health symptoms due in part to the condition of anonymity, reduced stigma, and the \ac{CA}'s capacity to build rapport with users. 
        
        DeVault et al. likewise designed a virtual human interviewer to assess mental health conditions including depression and anxiety via automated analysis of verbal and non-verbal behaviors~\cite{devault2014simsensei}. Findings from this user study suggested that the system was able to engage users in an open-ended conversation as long as $15-25$ minutes in duration, and that participants were comfortable sharing intimate information. Finally, Philip et al. contrasted the diagnostic performance of an embodied \ac{CA} and a psychiatrist in relation to major depressive disorder among $179$ outpatient participants~\cite{philip2017virtual}. Results from this proof of concept study suggest that \ac{CA}s might also serve as future means to conduct standardized clinical interviews.
        
        Many of these studies point to the self-report of mental health as a feasible use-case for the deployment of speech-enabled \ac{CA}s, yet are conducted in lab settings and specific to the design of embodied \ac{CA}s. Recent advancements in speech recognition technology, \ac{NLP} and the accessibility of smartspeaker devices such as Google's Home and Amazon's Echo have introduced the possibility of enabling and studying more `natural' conversational interactions in home settings, as others have begun to explore.
     
        Kocielnik et al. for example, designed an Amazon Alexa Skill named `Robota' to support work activity journaling; asking users to provide ten open-ended daily reflections~\cite{Rafal2018Workplace}. The authors examined how speech-enabled interaction affected workers' reflections and self-learning in comparison to a chat-bot employing the same questionnaire. Results from a three-week controlled field study showed that speech interaction enabled users to step back and reflect on their work as well as provided opportunities for workplace-related behavior-change, despite many technical limitations. In related work, Quiroz et al. developed an Alexa Skill enabling users to better express their emotions, complete self-assessments for depression and anxiety, and receive suggestions for improving their current state-of-mind~\cite{quiroz2020alexa}. While the results from this pilot study showed that participants were willing to engage with and trusted the agent when sharing personal information such as depression and anxiety scores, user experience scores indicated that participants considered the system to lack efficiency and novelty. Finally, and in related work, Motalebi et al. examined barriers to the use of Amazon Alexa to implement clinical therapy for patients with \ac{PTSD}; emphasizing the importance of short dialogues and interactivity for effective therapeutic content delivery~\cite{motalebi2018ptsd}.
        
        Researchers have therefore also begun to explore the adaptation of commercially-available systems to provide more accessible and conversational in-home experiences, including the design and use of \ac{CA}s to support the self-report of mental health and wellbeing `in-the-wild'. Few studies have yet however, to generate knowledge of the lived experience of users in relation to the practice of self-report via \ac{CA}, nor their perceptions of the use of these systems for this purpose. This knowledge is required to support the ethical and sustainable use of \ac{CA}s among users with \acp{AD} in particular, and yet must be approached in light of the present limitations of these technologies.
        
        \subsubsection{\ac{CA}s \& Their Limitations}\label{sec:ca_limitations}
    
            While speech-enabled \ac{CA}s are gaining widespread adoption in daily use~\cite{edisonresearch2020,Voicebot2019,chung2018health}, we cannot speak about the potential of \ac{CA}s to support practices of self-report without mentioning the current limitations of these technologies, which are often significant in nature~\cite{shneiderman2000limits, suhm2003towards, moore2013spoken, Gulfbetwee2016ewa, onceakind2019minji}.

            First, speech recognition technology is still in its infancy and cannot fully interpret or recognize users' utterances. A recent assessment of automatic speech recognition performance in psychotherapy discourse using Google Cloud's Speech-to-Text service reported a transcription error rate of 25\% in general conversation. For depression-related utterances, a sensitivity of 80\% was reported along with a positive predictive value of 83\%, and for clinician-identified harm-related sentences, the word error rate was 34\%~\cite{miner2020assessing}. While the authors of this study cautiously suggest that the technology may be feasibly adopted in psychotherapy, numerous other studies have reported instances in which \ac{CA}s misinterpret or fail to recognize user utterances causing confusion in the conversation and leading to further errors~\cite{myers2018patterns, suhm2003towards, pyae2018investigating}.
            
            Secondly, while some \ac{CA}s can mimic human-to-human-like conversations, none are fully capable of engaging in dynamic conversation~\cite{onceakind2019minji}. Typically, users have to know in advance what they can or cannot say to the \ac{CA}~\cite{pradhan2018accessibility, corbett2016can, Learnability2017Furqan} which often makes it cognitively demanding for users to interact~\cite{pradhan2018accessibility}. Moreover, current commercially-available \ac{CA}s allow a maximum of $12$ seconds for each user's response~\cite{Rafal2018Workplace, pearl2016designing}, and do not understand pauses in users' utterances which limits open-ended opportunities for self-report and dialogue.
            
            Thirdly, the monotonous and robotic voice of \ac{CA}s can undeniably impede users' engagement~\cite{miner2016smartphone}. And finally, studies have shown that users often establish high expectations for \ac{CA}s~\cite{Gulfbetwee2016ewa}, anticipating emotional exchanges, relationship building, and human-to-human like conversations~\cite{onceakind2019minji}, which often leads to a drastic decline in use when the \ac{CA}s cannot meet users' expectations~\cite{Gulfbetwee2016ewa, onceakind2019minji}. 
            
            Several researchers have provided initial guidelines to address these limitations in support of the design of effective \ac{CA}s~\cite{suhm2003towards, wei2018evaluating,clark2019makes, murad2019revolution, langevin2021heuristic}, including for health and wellbeing management~\cite{shin2018designing}. Studies also report that users themselves apply tactics including hyper-articulation and exaggeration, increased volume, use of different utterances or simplified words, and reformulation strategies, such as addition or substitution, removal, and re-ordering of words to alleviate conversational barriers~\cite{myers2018patterns, mathias2016how, cheng2018doesn}. 
            
            It is currently unclear however, of the extent to which these guidelines for the design of effective dialogue and for overcoming \ac{CA}s' limitations apply to the mental health context. We know little about how people living with mental illness including \ac{AD}s might engage with \ac{CA}s in light of these current technical limitations, nor in support of the self-report of their mental health and wellbeing. To address these research gaps, we design and conduct an `in the wild' field study. 
            
% UNUSED NOTES
% In addition to qualities, \ac{CA}s based on speech interfaces offer more convenient and `natural' forms of interaction. Evidence suggest that speech, as the primary mode of interaction, can significantly improve user engagement --- a quality vital for any effective mental health technology~\cite{borghouts2021barriers,torous2014patient}.
% \ac{IVR} and \ac{ASR} based systems are some of the earliest examples of speech-enabled self-report technologies, and have been successfully deployed in a number of real-world contexts. ~\citet{azzini2003automated} for example, developed a telephone-based dialog system to enable hypertensive patients to record their symptoms by calling a toll-free number, thereby mitigating the need for a clinical visit. ~\citet{levin2006evaluation} evaluated the usability of an \ac{ASR}-based `Pain Monitoring Voice Diary' system, finding that users were able to navigate the flexible interface, and that self-reporting efficiency increased with users’ experience, both in terms of session duration and avoidance of troublesome dialog scenarios.
% Others have explored applications of such technologies to support mental health. ~\citet{hudlicka2013virtual} developed a multi-modal embodied system named `Virtual Mindfulness Coach' and evaluated its effectiveness in training and coaching mindfulness meditation. Results from this study demonstrated that students achieved a higher sense of self-efficacy concerning the establishment of regular mindfulness practice through coach-based training than via self-administered training using written and audio materials.~\citet{fitzpatrick2017delivering} developed a fully automated chatbot named `Woebot' and studied its feasibility, acceptability, and efficacy in delivering a self-help \ac{CBT} program for college students with symptoms of anxiety and depression. Findings of this study showed that delivery of \ac{CBT} through the chatbot significantly reduced participants' symptoms of depression and anxiety. The authors suggest that process factors were more influential on participants' acceptance of the system than content factors, mirroring traditional therapy.
% Other researchers have focused on the social dimension of \ac{CA} technologies. Miner et al., for example, compared sentiment tendencies and mirroring behaviors in human-human and human-\ac{CA} dialogues in mental health settings by employing Relational Frame Theory~\cite{miner2016conversational}. They found that while human sentiment-related interaction norms persist in human-agent dialogues, inhibition towards use of obscenity was greatly reduced. 
% \subsection{Mobile Mental Health Self-report Technology [Omit if doesn't fit]}
% Effective monitoring of symptoms of mental illness can help patients recognize so-called `early warning signs' resulting in timely clinical interventions~\cite{bardram2013designing}. 
% Monitoring and assessing of mental health is often based in patients' self-reported experiences~\cite{doherty2020design}, whether in the form of elicitation or feedback~\cite{carter2005ema}. 
% Eliciting patient's qualitative reflection on their mental state(e.g., mood, stress, or other symptoms) via interview or diary, and quantitative feedback to a clinically validated questionnaires (e.g.,~\ac{WHO-5} and \ac{PHQ-9}) are two most common methods of monitoring and assessing mental illness.
% Qualitative interview support greater freedom of expression and allow users to provide open-ended forms of information on their attitudes, thoughts, and actions~\cite{runyan2013smartphone}. In contrast, self-reports on health questionnaires requires patients to respond to a pre-determined set of questions.
% These methods are often used together to gather a holistic and deeper insights into patient's health and well-being~\cite{khan2008reconexp, cherubini2009refined}. 
% Traditionally, these self-reporting methods are administered using pen and paper. With the advancement in mobile technology and widespread use of the internet, much research has explored self-report technologies in digital tools (e.g.,smartphones and web applications) using \acp{GUI}. 
% Significant developments in this area include for example, `MONARCA' system, which allowed patients with bipolar disorder to self-monitor their wellbeing by self-reporting their mood, sleep, and medication among other health related data~\cite{bardram2012monarca}. The fourteen-week field trial of the system showed improved adherence to self-assessment, patients considered the system easy to use, and the perceived usefulness of the system was rated high compared to using paper-based forms~\cite{bardram2013designing}.
% ~\citet{doherty2012engagement} developed `Mind Balance' which enabled people with depression to share `personal stories' as a part of
% online \ac{CBT} program. Findings from a clinical trial of the system showed increase in treatment adherence and an overall decrease in participant's depressive symptoms.
% In 2018,~\citet{doherty2018mobile} designed `Brightself' mobile app to support longitudinal self-report of antenatal mood and depression. Results from the nine-months feasibility study of the app demonstrated that the system enabled extended care, overcome stigma, support disclosure, and foster trust between patients and health professionals by engaging women in the self-report of wellbeing and depression during pregnancy.
% Recent mental health apps have taken a form of `Pocket Psychiatry'~\cite{anthes2016pocket} providing wide range of mental 
% ~\citet{rohani2020mubs} in 2020, designed `MUBS' that supported~\ac{BA} for the patients with affective disorder by providing activity recommendations based on sensor data and patient self-reports.
% While these tools are considered efficient, self-report using smartphone technology presents a variety of significant challenges in general, including user engagement, reporting burden, data validity and honest disclosure among others~\cite{doherty2019engagement, berkel2018ema, harari2016using}, which speech-enabled \ac{CA}s may have the potential to address.
% \subsection{\ac{CA} privacy concerns}
% Stills needs ro be added
% \subsection{\ac{CA} in social context }
% Stills needs ro be added
% have been successfully deployed for collecting patient self-reports to monitor chronic diseases. 
% While these studies provide valuable insight into the usability of the speech-enabled \ac{CA}s in health self-reports, such as  have made these systems and allowed researchers to investigate their use in home settings using off-the-shelf devices.
% Whilst these initial research efforts suggest \apc{CA}' potential to support the self-report of mental health and well-being, there is very little understanding of how users with mental illness perceive these systems, given the limitations of \ac{CA}s as described next.
% as described next, in an attempt to understand the perspectives of people with the affective disorder on self-reporting experiences of mental health and wellbeing through a \ac{CA}.
% research has investigated the ways to improve communication breakdowns and speech recognition errors to reduce the gap between users' expectations and experiences in using \ac{CA}s.
% exploring therapeutic applications of the \ac{CA}s for people living with \ac{AD}s and found positive results in terms of its feasibility in improving mental health outcomes~\cite{hoermann2017application, vaidyam2019chatbots}.
% Results from the study demonstrated \ac{CA}s as  validity and acceptability for diagnosing major depressive disorders and showed that \ac{CA}s are promising tools for the conduct of standardized and well-accepted clinical interviews.
% While it is possible living with \ac{AD}s may have the most to benefit from these speech-enabled technologies, without the knowledge of its use in the real-world, we fail to truly understand users' perception, and the factors that influence user experience. 
% Despite the growing adoption these commercially-available \ac{CA}s for heath and wellbeing, we know little about users' lived experiences and the perceptions of \ac{CA}s for the self-report of mental health and wellbeing. 
% And yet realizing this potential also hings upon the design of appropriate conversations; a far from simple-task
% nor which \ac{CA} characteristics they view as important for sustainable self-reporting practice.
% Despite the current limitations of \ac{CA} technologies, a present lack of knowledge concerning users' experiences, and ethical and design challenges, it is also possible that individuals experiencing mental illness may have the most to benefit from speech-enabled self-report technologies. 