\section{Discussion}

    This paper presents initial insight into the experiences of people living with \ac{AD} of a speech-enabled \ac{CA} for the self-report of mental health and wellbeing in an at-home setting. Our findings indicate that \ac{CA}s' conversational features have the capacity to support engaging and sustainable self-report experiences, revealing users' experiences as strongly shaped by strategies adopted to overcome \ac{CA}s' technical limitations, diverse personified perceptions of the agent, the socially-contingent nature of self-reporting practice, and users' reflections on privacy and security concerns.

    In light of these findings, we further reflect on the challenges of deploying \ac{CA}s in the homes of users with \ac{AD}, and discuss implications for the future design of conversational agents to support the self-report of mental health and wellbeing (See Table~\ref{tab:challenges}).
    
    \input{tables/_challenges}
    
    % Challenge 1 from theme 1
    \subsection{The Challenge of Conversational Pattern Matching} % Machine

        Participants in this study demonstrated high levels of engagement in the practice of self-report and with \acl{app} throughout the 4-week period (See Section ~\ref{sec:participant_engagement}); many describing \acl{app} as efficient, easy to use and an attractive medium for the self-report of mental health and wellbeing. This, despite frequent recounting of moments of frustration, disappointment and even occasional anger in relation, most often, to the technical limitations of the technology. Participants recounted as particularly frustrating those limitations of the \ac{CA} experience which presented barriers to their self-expression; including the imposition of an 8 to 12 second time limit on their responses. 
        
        Much prior \ac{HCI} research has highlighted the challenge of user burden in the design of diverse self-report technologies \cite{harari2016using, van2017experience, doherty2020design}. And yet what was therefore most surprising in this instance were the lengths to which participants went, often spontaneously, to adopt strategies to maintain their engagement; from making multiple entries, to engaging in long periods of prior reflection, to providing brief responses in the moment (See Section~\ref{sec:strategies_to_overcome}). 
        
        These acts of perseverance and persistence demonstrate a willingness to adapt to the conversational patterns of an agent that one might expect during conversation with a human interlocutor. In this instance, participants were required to adapt to machine speech and dialogue patterns in service of the conversation. In future, a conversational agent might do likewise - leveraging this parallel human capacity to adapt to and match conversational patterns - in order to keep the conversation going, in the moment and over time. 
        
        This is a task we highlight as not only future but present design challenge; suggesting that designers might not only work to overcome \ac{CA} limitations but design for adaptation, and in turn sustainable and engaging self-report experiences, within these constraints. In line with participants' comments (See Section~\ref{theme:design_recommendations}), this might entail first and foremost designing for a conversational pace and pattern congruent with the system's capabilities, by, for example tailoring opportunities for self-expression through continuous probing and guiding of the conversation.
        
    \subsection{The Challenge of Filling the Right Gap} % Designer / User
    % Ethical Ramifications of \ac{CA} Personifications
    
        Participants of this study recounted not only diverse strategies for interaction but diverse personifications of an agent; from blank slate to machine, friend, good listener, companion, and tool for emotional venting and self-talking. Although a participant group whom might be classified as vulnerable in regards to their mental health, participants of this study therefore also demonstrated significant resilience in relation to their ability to appropriate the technology to fill the unmet gap in social interaction that they felt it might best --- even were that simply `being listened to'. 
        
        We were surprised, as authors, at the diverse forms of value participants found (and often creatively so) in a technology comprising a number of technical limitations. This practice of appropriation, while intriguing, also represents a significant challenge for the designer who might wish to provide a consistent user experience. For any attempt to shape such intense appropriation of a technology is additionally complicated by the contingency of the act of personification upon both human and machine.
        
        We might then strive instead to support diverse appropriation of these systems in line with users' own needs --- particularly in the context of design for the subjective, personal and interpersonal experience of mental health and wellbeing. Participants in this study, for example, shared a sense of being heard while talking to \acl{app} - a quality fundamental to realizing sustainable long term \ac{CA}-user relationships and which could prove additionally beneficial for encouraging positive behavior change as reported in prior research (e.g.,~\cite{thieme2015designing, bickmore2005establishing}) - if authentically felt and granted.
        
        Personification of technology also raises however ethical questions in relation to the deployment of systems for the purposes of social interaction; from possibilities for stigmatization to disconnectedness. And, deploying CAs for the self-report of mental health and wellbeing ethically will therefore also require attending to and providing space for human experiences of sociality, connectedness, empathy and compassion, while allowing users to appropriate technology in the ways they see fit.

    \subsection{The Challenge of The At-Home Social Context} % Designer
    % Establishing User-\ac{CA} Trust} 

        The at-home context in which this system was deployed led to a variety of unexpected findings related to the extent to which not only use of the \ac{CA} was shaped by social context but how the \ac{CA} shaped social context and even intimate relationships. This poses both challenges and opportunities for designers when it comes to the integration of CAs within the unique social context of home-use. The unpredictability of this context casts doubt on and raises ethical challenges for the enactment and shaping of new possibilities, from opportunities for family members to demonstrate care to creating confrontations with and through data.
        
        Introducing a system for the self-report of mental health into a home context must itself be considered an act of vulnerability requiring trust and courage, and shaped by perceptions of \ac{CA} as harmless machine or eavesdropping device. In this study, we found that participants' privacy concerns primarily stemmed from their distrust of the smart-speaker devices. As a measure to protect their personal privacy, many participants turned off the speaker when not in use, while others held back during self-reporting. Although similar findings have been reported in prior studies (e.g.,~\cite{pradhan2018accessibility, lau2018alexa}), the need for users with mental illness to trust the technology is even more significant as privacy and data security concerns could have additional adverse effects on their mental health and well-being (See Section~\ref{sec:eavesdropping}). 
        
        We therefore suggest transparent communication of \ac{CA} privacy policies and data practices in addition to educating users about privacy settings, in order to build trust between users and these systems. Participants were made aware of the data practices adhered to in this study, including what data was to be collected and who would have access (See Section ~\ref{sec:pre_study}). Providing this information enabled participants to interact with the system according to their own discretion. While the designers of \ac{CA}s cannot speak for the privacy policies and data practices of smart-speaker manufacturers and operators, they can support users' privacy concerns by educating them about the privacy settings of these devices in order to help build trust between users and their devices. % For example, in addition to being transparent about the \ac{CA}'s own privacy and data practices, it can also provide information about the sources of smart speakers' privacy settings (e.g.,~\cite{alexa_privacy,google_privacy}).



% -------

% R3: How do personal privacy and data security concerns lead to distrust in CA technology for reporting mental health status? As authors have already provided the quotes on cautious use of CA technology, 
% what would be future design implications to support users' trust, privacy and security concerns.  
% 'my app friend'
% Design challenges related to the finindgs of our thematic analysis relfecting the key threads of users' experiences, and the design choices in support of sustainable engaging experiences
% Participants in this study were faced with the challenge of adopting personal strategies in line with their personal preferences for conversational interaction. 
% in order to mitigate the need for users to make multiple entries or prepare in advance to self-report their wellbeing fully.
% we suggest as one means of navigating current technical limitations in support of a fluid conversational experience, design to suppor
% While these strategies highlight participants' willingness to persist in their use of the system, they nonetheless highlight burdensome interactions --- one of the most frequently discussed design challenges in this domain of \ac{HCI} research~\cite{harari2016using, van2017experience, doherty2020design} --- meriting future design focus. 
% \subsubsection{Design for An Appropriate Conversational Pace}
% In addition to advice for design for conversation and reflection
% Enable Continuous Interaction by Appropriate Probing \& Guiding
% We suggest that designers might not only work to overcome these limitations but design to realize positive self-report experiences in light of these current system features. 
% In this instance, the machine as conversational partner had little capacity to respond in kind in order to maintain the conversation, and
% While such connections and associations may serve as a means of overcoming stigma for people with \ac{AD}s as they often tend to have less social interactions due to the social stigma attached their mental condition, they]]]]]]
% As these devices grow ever more ubiquitous and technologically advanced to engage users in social conversations, there is potential for this vulnerable population group to become over-attached and even dependent to these systems, further isolating them from their social circle and distancing them from their personal relationships~\cite{vaidyam2019chatbots}. 
% In turn, the technology itself could be stigmatized. 
% Based on participants narrative in this study, we suggest imbuing conversational characteristics to support social goals and empathy. 
% However, designers also have to be careful about the potentially negative impact of such design choices as discussed above.
% \subsubsection{Imbuing Conversational Characteristics to Users' Support Social Goals}
% \subsubsection{Imbuing Conversational Characteristics to Support Empathy}
% Design for Adaptation
% Participants in this study not only used \acl{app} as a tool to self-report their mental health and wellbeing, they expressed that they wanted the illusion that the \ac{CA} cared although they did not expect \ac{CA} to understand their emotions. \ac{CA}s with sensitivity to the context of interaction, including the sentiment of the users' utterance, the topic of the conversation and ability to formulate relevant follow-up questions based on user response could enable such conversational interactions~\cite{clark2019makes}.
% Many participants in the study appreciated \acl{app}'s response feedback and mentioned that it gave them a sense of being heard, although that was not the intention of the design as described in Table~\ref{tab:diary_mgmt}. 
% Participants' desire for such emotional support from the agent suggests the need for designing \ac{CA}s with ability to formulate empathetic responses to user utterances. 
% With the possibility of tailoring voice features such as tone, intonation, speed and pitch, speech-enabled \ac{CA}s could emulate an empathetic self-reporting experience. 
% Related work in text-based \ac{CA} demonstrated that such interactions could have positive effects on users' mental health and wellbeing~\cite{inkster2018empathy}. 
% \subsubsection{Transparency on Privacy and Data Practices to Support Trust}
% They can be more transparent on their own data practices
% \subsubsection{Supporting User Agency in Terms Data Sharing}
% \subsubsection{Educating Users About Privacy Settings}
% CAs can provide guides on how to turn off data sharing in smart speakers
% As such, some held back on sharing sensitive information, indicating in turn that participants of this study felt sufficiently able and assertive to establish bounds on the use and sharing of their data in line with their own levels of comfort and trust. 



% -------



% UNUSED NOTES
% R1: It would be helpful to have some list or table with the issues you identified through the interviews. Maybe you could come up with a table that brings together the identified issues and the possible solutions.
% Issue: What is the issue? | Why it is an issue | What might be the cause? | How to solve/mitigate the issue?
% Challenge 1 from Theme 1
% \subsection{Overcoming the Challenge of Reporting Burden using CAs}
% Why it is an issue? 
% The burden associated with the use of self-report technologies is one of the most frequently discussed design challenges in this domain of HCI research~\cite{harari2016using, van2017experience, doherty2020design}. It has been suggested that CAs, as alternative media to paper, mobile and web technologies, may have the capacity to overcome this challenge, in part by serving as a more intuitive and engaging mode of interaction. Our findings suggest both that this may be the case, and that this challenge is indeed complex.
% Participants in this study demonstrated high levels of engagement (See Section ~\ref{sec:participant_engagement}), describing \acl{app} as efficient, easy to use and an attractive medium for the self-report of mental health and wellbeing. And yet their experience was also not itself without its challenges; highlighting new kinds of burden associated with the limitations of CA technology.
% Participants recounted as particularly frustrating those limitations of the CA experience which presented barriers to their self-expression; including the imposition of an 8 to 12 second time limit on their responses. Participants spontaneously adopted tactics to overcome these challenges; making multiple entries, adapting to the \ac{CA}'s speech patterns, and preparing their own responses in advance (Section~\ref{sec:strategies_to_overcome}).
% While these strategies highlight participants' willingness to persist in their use of the system, they nonetheless highlight burdensome interactions meriting future design focus. We suggest that designers might not only work to overcome these limitations but design to realize positive self-report experiences in light of these current system features. % It's about designing around limitations - casting a negative as a positive?
% In order to alleviate this unintended self-reporting burden, we suggest improving \ac{CA}'s conversational skills by probing users' responses further and guiding conversation more.
% \subsubsection{Design for An Appropriate Conversational Pace}
% Enable Continuous Interaction by Appropriate Probing \& Guiding
% In line with participants' comments (See Section~\ref{theme:design_recommendations}), we suggest as one means of navigating current technical limitations in support of a fluid conversational experience, design to support dialogue flow at a pace matching the system's capabilities. This may also entail providing frequent opportunities for self-expression by continuously probing and guiding the conversation in order to mitigate the need for users to make multiple entries or prepare in advance to self-report their wellbeing fully.
% At the same time, users' comments suggest alternate means of designing for valuable interactions --- whether a once-a day 10 second recap as a means of inspiring concise expression and reflection, or a guided reflective practice focused on eliciting positive experiences of daily life as part of a therapeutic intervention
% While these recommendations align with prior \ac{CA} design guidelines (e.g.,~\cite{murad2019revolution, langevin2021heuristic}),
% ~\cite{tindall2017behavioural} than how it is interpreted in some of the well established heuristics for designing \ac{CA}s~\cite[Table 2. G10]{murad2019revolution}~\cite[Table 8]{langevin2021heuristic}. 
% The \ac{CA} prototype used in this study asked only three questions in a session which often restricted users from expressing their emotions fully. 
% the context of the self-report of mental health involving the vulnerable population group is worth noting. 
% Challenge 2 from Theme 2 and 3
% \subsection{\ac{CA}-User Relationship Framing}
% Participants' involvement in this study required supporting high levels of trust given the potentially vulnerable nature of the population group, the stigma surrounding mental illness, and the novel nature of the technology itself. Participants' perceptions of \acl{app} as a good listener, a companion, and a tool for emotional venting and self-talking suggest \ac{CA}'s potential in fulfilling user's unmet gap in social interactions. Participants also shared their feeling of being heard while talking to \acl{app} -- a quality that we believe is fundamental for sustainable long term \ac{CA}-user relationships which could be beneficial for encouraging positive behavior change as reported in prior research (e.g.,~\cite{thieme2015designing, bickmore2005establishing}). Such connections and associations may also serve as a means of overcoming stigma for people with \ac{AD}s as they often tend to have less social interactions due to the social stigma attached their mental condition as reported by the participants in this study. 
% While the personified perceptions suggest the potential of \ac{CA}s to prove of meaningful value to users, they also raises questions concerning the potential ethical ramifications. As these devices grow ever more ubiquitous and technologically advanced to engage users in social conversations, there is potential for this vulnerable population group to become over-attached and even dependent to these systems, further isolating them from their social circle and distancing them from their personal relationships. In turn, the technology itself could be stigmatized. 
% Based on the narratives of the participants in this study, we suggest imbuing conversational characteristics to support social goals and empathy.
% \subsubsection{Imbuing Conversational Characteristics to Users' Support Social Goals}

% Participants in this study not only used \acl{app} as a tool to self-report their mental health and wellbeing, they expressed that they wanted the illusion that the \ac{CA} cared although they did not expect \ac{CA} to understand their emotions. Research suggests that \ac{CA}s with sensitivity to the context of interaction, including the sentiment of the users' utterance, the topic of the conversation and ability to formulate relevant follow-up questions based on user response could enable such conversational interactions~\cite{clark2019makes}. However, designers also have to be careful about the potentially negative impact of such design choices as discussed above.
% \subsubsection{Imbuing Conversational Characteristics to Support Empathy}
% Many participants in the study appreciated \acl{app}'s response feedback and mentioned that it gave them a sense of being heard, although that was not the intention of the design as described in Table~\ref{tab:diary_mgmt}. Participants' desire for such emotional support from the \ac{CA} suggests the need for designing agents with ability to formulate empathetic responses to user utterances. With the possibility of tailoring voice features such as tone, intonation, speed and pitch, speech-enabled \ac{CA}s could emulate an empathetic self-reporting experience. Related work in text-based \ac{CA} demonstrated that such interactions could have positive effects on users' mental health and wellbeing~\cite{inkster2018empathy}. 
% Challenge 3 from Theme 4
% \subsection{Privacy \& Data Security} 
% Why this is an issue?
% Participants in this study often reported that their privacy concerns did not only lead to anxiety and paranoia, it also demotivated them to fully express their emotions. As a measure to protect their personal privacy, they often turned off the smart speaker when it was not in use, others held back on sharing sensitive information on their self-reports. These privacy seeking behaviors echo prior findings that advocate transparency on \ac{CA}'s privacy policies and data security practices to build trust between the users and \ac{CA}s~\cite{lau2018alexa, pradhan2018accessibility}. The need for users with mental illness to trust the technology is even more significant as privacy and data security concerns could have adverse effects on their mental health and well-being, as reported in Section~\ref{sec:eavesdropping}. 
% \subsubsection{Transparency on Privacy and Data Security to Support Trust}
% Participants in this study were made aware of the data practices adhered to in this study, which included what data is collected and who has the access to it (Section ~\ref{sec:pre_study}). Transparency on such information enabled participants to take necessary steps to use the system on their own discretion. As such, some held back on sharing sensitive information on their self-report indicating that the participants in this study felt sufficiently able and assertive to establish bounds on the use and sharing of their data in line with their own levels of comfort and trust. 
% \subsection{Potential Benefits and Risks of Using \ac{CA}s for the Self-Report of Mental Health \& Wellbeing}
% \subsubsection{Benefits}~\label{sec:benefits}
% +1
% efficiency + engagement => positive perception + less burden
% As a primary benefit of using \ac{CA}s for mental health self-report, participants of this study demonstrated high levels of engagement (See Section ~\ref{sec:participant_engagement}), which often reflected broad acceptance of the technology as well as positive perceptions of the \ac{CA}'s potential to support mental health and wellbeing. In particular, speech as an efficient, easy to use and attractive mode of self-report could potentially alleviate reporting burden on the users, which is one of the biggest challenges of the current self-report technologies~\cite{harari2016using, van2017experience, doherty2020design}. 
% +2 
% Natural medium + friendly voice => better self-report experience + therapeutic experience + feeling heard
% Participants in this study reported that it allowed them to express their emotions more freely and spontaneously compared to other means of self-report indicating that speech as a more natural form of interaction could help improve the self-reporting experiences. Likewise, \ac{CA}'s anthropomorphic characteristics could induce a therapeutic self-reporting experience by responding to users with empathy in calming, friendly and caring voice. As such, participants in this study also spoke to feeling heard -- a quality that we believe is fundamental for sustainable long term \ac{CA}-user relationships which could be beneficial for encouraging positive behavior change as reported in prior research (e.g.,~\cite{thieme2015designing, bickmore2005establishing}).
% +3 
% % personification => social companion + overcoming stigma
% Participants' involvement in this study required supporting high levels of trust given the potentially vulnerable nature of the population group, the stigma surrounding mental illness, and the novel nature of the technology itself. Participants' perceptions of \acp{app} as a good listener, a companion, and a tool for emotional venting and self-talking suggest the potential of \ac{CA}s to prove of meaningful value to users and shows \ac{CA}'s potential in fulfilling user's unmet gap in social interactions. Such connections and associations may also serve as a means of overcoming stigma for people with \ac{AD}s as they often tend to have less social interactions due to the social stigma attached their mental condition as reported by the participants in this study.
% \subsubsection{Potential risks}\label{sec:risks}
% % -1
% % 12 sec limitation = frustration/stress + expectation gap + adverse effect on mental state
% One of the potential risks of using \ac{CA}s for the self-report of mental health stems from the \ac{CA}'s technical limitation that prevented users from completing their open-ended self-reports. Due to this restriction, participants in this study reported frustrating and disconnected self-reporting experiences despite setting a realistic expectations before \ac{CA} use. Prior studies~\cite{Gulfbetwee2016ewa, onceakind2019minji} have reported similar gap between users' expectation and \ac{CA}'s capacity to engage in a human-to-human-like conversation. As experienced by some participants in this study, such mismatch between users' expectation and \ac{CA}'s capacity could add unwanted stress to their mental state and adversely affect their wellbeing. Nonetheless, to overcome \ac{CA}'s constraints on the open-ended self-report, many participants employed tactics such as making multiple entries, adopting to \ac{CA}'s speech, and preparing in advance. While the tactic of adopting to \ac{CA}'s speech has been reported in prior research~\cite{myers2018patterns}, making multiple entries and preparing in advance to self-report indicate users' willingness to use the technology despite its limitations. Many participants in this study found these strategies feasible and applied them to fully express their emotions suggesting that it is important to resist a narrow focus on what the system can do, and to think about how the technology best supports participants' own health and wellbeing goals, and how users are best able to appropriate the technology into their own lives. However, although it was not stated, the requirement of these additional actions to appropriate the technology undoubtedly added a burden on the users, which undermines the purpose of these systems as an alternative to traditional methods of self-report.
% % -2 and -3
% Participants' descriptions of \acl{app} in this study ranged from `this round little thing' to `this little person in my life;' more often than not leaning in the direction of humanization and personification of the agent. Some expressed that they would trust \ac{CA}s more than their human counterparts as the \ac{CA} made them feel heard. While these associations create opportunities for building therapeutic relationships between \ac{CA} and the users, it also raises questions concerning the potential ethical ramifications. As these devices grow ever more ubiquitous and technologically advanced to engage users in social conversations, there is potential for this vulnerable population group to become over-attached and even dependent to these systems, further isolating them from their social circle and distancing them from their personal relationships. In turn, the technology itself could be stigmatized. Therefore, as designers and developers of these systems we are increasingly called upon to consider questions pertaining to the kinds of relationships we hope to embody and support via these systems, and the expectations we intend to set for users in this respect. Do we aspire to realize, for example, professional, casual, therapeutic or wholly transactional relationships, and which design choices realize these modes not only of interacting, but relating?
% % -4
% % privacy and security => paranoia/anxiety + bad quality of self-report
% Another potential harm of using \acp{CA} for mental health self-report concerns privacy and data security issues associated with \ac{CA}s. As reported by some participants in the study, their privacy concerns did not only lead to anxiety and paranoia, it also demotivated them to fully express their emotions. However, many participants felt sufficiently able and assertive to take the steps necessary to establish bounds on the use and sharing of their data in line with their own levels of comfort and trust. 
% The transparent data practices adhered to in this study by informing the users before they participated in the study that the transcripts of their self-reports would be accessible to the primary researcher of the study.
% Aligned with prior findings on users' privacy seeking behavior while using \ac{CA} (e.g., ~\cite{pradhan2018accessibility, lau2018alexa}), some participants in the study turned off the smart speaker when not in use. Yet others held back on sharing sensitive information on their self-report. Such appropriation of the technology into their lives and for their own purposes despite privacy concerns therefore reflects diverse forms of value associated with the technology.
% This is particularly important for users living with \ac{AD} and other vulnerable populations.
% These benefits and potential risks of adopting \ac{CA}s for the self-report of mental health and wellbeing brings us to the question; what kind of futures do this population group envision for this emerging technology? 
% These potential risks of adopting \ac{CA}s for the self-report of mental health and wellbeing suggests the need for improving conversational design to make use of this emerging technology.
% \subsection{Stigma, Vulnerability \& Design Ethics}
%  Participants' involvement in this study required supporting high levels of trust given the potentially vulnerable nature of the population group, the stigma surrounding mental illness, and the novel nature of the technology itself.
% Participants' perceptions of \acp{app} as a good listener, a companion, and a tool for emotional venting and self-talking suggest the potential of \ac{CA}s to prove of meaningful value to users, and yet also raises questions concerning the potential ethical ramifications of \ac{CA} personification. Such connections and associations may, for example, serve as a means of overcoming stigma, or strengthening it, should that stigma become attached to the technology itself. % \ac{CA} Interventions reduced positive and negative volitional stigma but not traditional stigma (social distance)~\cite{sebastian2017changing}
% It is also therefore important to question the extent to which these agents are truly capable of serving as companions, and indeed the extent to which we desire them to do so. The therapeutic, and healthcare literature more broadly, highlights the role of relationship and rapport as mediating factors in the efficacy of care. Can a \ac{CA}, for example, play the role of a therapist? This research suggests that users may find value in connecting to agents to a greater extent than even we as developers imagined given the limited nature of this agent, and yet also underlines the need for future research if these technologies are to be implemented and deployed in ethical ways.
% \subsection{Conversational Self-Report | A Relationship-Oriented Design Framing}
% One of the as-of-yet unspoken questions underlining this framing of technology concerns whether self-report is best framed as a conversation at all. And indeed, where the value lies in doing so. The term `report' may be read as implying a power imbalance which did not necessarily reflect how all participants of this study conceived of their relationship to \acl{app}. 
% Indeed, participants' descriptions of \acl{app} itself ranged from `this round little thing' to `this little person in my life;' more often than not leaning in the direction of humanization and personification of the agent. As these devices grow ever more ubiquitous, researchers are beginning to question, as in this work, ways of understanding and designing for increasingly meaningful and longitudinal forms of interaction. This is not a trivial change in orientation, and one which has featured strongly in our thinking around and framing of the design of \acl{app}. As designers and developers of these systems we are increasingly called upon to consider questions pertaining to the kinds of relationships we hope to embody and support via these systems, and the expectations we intend to set for users in this respect. Do we aspire to realize, for example, professional, casual, therapeutic or wholly transactional relationships, and which design choices realize these modes not only of interacting, but relating?
% \subsection{Implications for Future Designs of \ac{CA}s to Support the Self-Report of Mental Health \& Wellbeing}
% % \subsection{Desired Futures} 
% % What else needs to be done to improve \ac{CA} design for self-reports of mental health and wellbeing?
% \begin{quote}
% \vspace{2mm}
% \textit{``\acl{app} can be someone to talk to\ldots I don't know if you have seen the movie, `Her'~\cite{spike2014her}. Something like that. The `Her' thing is not to automate your life or to remind you anything. Google Assistant can do that. Someone to talk with, not to get any feedback, just to review your day\ldots''} [P16]
% \vspace{2mm}
% \end{quote} 
% Looking forward, as speech-enabled devices become more mainstream, it is worth considering the futures we desire, and how we might design to sustainably support them. What role do we want these devices to play in our lives? 
% Although many participants expressed hesitation in considering health recommendations provided by a \ac{CA}, several mentioned that they would welcome non-intrusive recommendations that reminded them of the things they could do, although added that a \ac{CA} should grant them broad scope to develop and make their own decisions; \textit{``You can remind sometimes. Like `Do you want to do a breathing exercise to improve your mental health?' and then you can say `Yes' or `No'. I think it's important that you still have the possibility to say no''} [P14]. Other participants suggested incorporating humor and health advice into conversations with \acl{app}.
% These possibilities raise additional questions. Do we, for example, 
% want agents to provide recommendations for our health and wellbeing, or 
% prove truly capable of holding rich conversations, or 
% might we prefer uncaring machines, who may also ironically be best placed to provide an experience of care?
% These are the kinds of questions raised by this study, as one of the first to engage such a participant group in the real-world use of \ac{CA} technologies, and we hope will serve to fuel future research efforts, as we continue to reflect on the implications of growing adoption of these technologies.
% Our results suggest the need for several considerations to design \ac{CA}s for the self-report of mental health. Reflecting on our findings, we envision future \ac{CA}s for mental health self-report as intelligent agents which can engage users in a wide range of interactions beyond collecting emotional self-reports. 
% % Here, we discuss implications of our findings for designing such \ac{CA}s.
% Here, we discuss potential ways to capitalize the benefits of these systems to support the self-report of mental health and wellbeing.
% \subsubsection{Transparency on Privacy and Data Security to Support Trust}
% Some participants in this study turned off the smart speaker when not in use. Yet others held back on sharing sensitive information on their self-report. Such appropriation of the technology into their lives and for their own purposes despite privacy concerns therefore reflects diverse forms of value associated with the technology.
% Results from this study echo prior findings that advocate transparency on \ac{CA}'s privacy policies and data security practices to build trust between the users and \ac{CA}s~\cite{lau2018alexa, pradhan2018accessibility}. The need for users with mental illness to trust the technology is even more significant as privacy and data security concerns could have adverse effects on their mental health and well-being, as reported in Section~\ref{sec:eavesdropping}. Participants in this study were made aware of the \ac{CA}'s data practices, including what data is collected, who has the access to it and how it is protected. Transparency on such information helped participants trust the system and enabled them to take necessary steps to use the system on their own discretion. It should also be noted that although privacy resignation is becoming more common and nuanced due to the pervasive collection of user data~\cite{lau2018alexa}, transparency on privacy and data practices can foster a positive relationship between \ac{CA} and users, promoting a longitudinal engagement with the system.
% \subsubsection{Transparency about \ac{CA}'s Limitations to Set Users' Expectations}
% Research suggests that due to the conversational nature of the interaction, users often tend to have higher expectations from the \ac{CA}s~\cite{Gulfbetwee2016ewa}. Participants in this study were informed about \acl{app}'s limitations to set users' expectations, which enabled them to appropriate the system by using tactics to overcome the limitations. The disclosure of the \ac{CA} limitations also alleviated otherwise frustrating self-reporting experience by filling the gap between their expectation and the \ac{CA}'s capacity.
% \subsubsection{Improving \ac{CA} Design by Varying, Probing and Guiding for Positive Conversation}
% Based on the experience using \ac{CA} for mental health self-report for $28$ days, participants in this study suggested to improve the conversation by varying the questions, probing their responses for more information, and guiding with appropriate response option (See Section~\ref{theme:design_recommendations}). Although these design recommendations may not be unique to this study, the context of mental health self-reports involving potentially vulnerable nature of the population group is worth noting. As such, the participants' suggestion to enable \ac{CA} to guide them to talk about positive things in their lives reflects more of a therapeutic intervention~\cite{tindall2017behavioural} than how it is interpreted in some of the well established heuristics for designing \ac{CA}s~\cite[Table 2. G10]{murad2019revolution}~\cite[Table 8]{langevin2021heuristic}. Likewise, varying questions and probing on the responses also needs to account for the users' mental state to engage them in a positive conversation.
% \subsubsection{Supporting (Verbal) Self-reflection}
% Prior research (e.g.,~\cite{kocielnik2018designing, myers2018patterns}) in speech-enabled \ac{CA} has mostly relied on \ac{GUI} to allow users to reflect on their data. While recommendation for accompanying mobile or web app self-reflection was common, participants' suggestion to incorporate verbal reflections of their wellbeing within the self-reporting experience is unique to this study. Participants perceived that the verbal reflection on their data could be more valuable than traditional \ac{GUI}-based tools in supporting health behavior change. Future research could look into designing for such reflection as it entails several challenges including positioning and automated vs. on demand provision of the reflection as discussed by the participants in this study (Section~\ref{sec:reflection}).
% \subsubsection{Imbuing Conversational Characteristics to Support Social Goals}
% Despite the understanding that the \ac{CA} is just a machine, participants' narratives in this study, including personification of the \ac{CA} in various forms indicate the benefits for imbuing conversational characteristics in \ac{CA} design to support users' unmet gap in social interactions. While the participants did not expect \ac{CA} to understand their emotions, they still wanted the illusion from the \ac{CA} that it cared, indicating the need for \ac{CA}'s sensitivity to the context of interaction including the sentiment of the users' utterance, topic of the conversation and ability to formulate relevant follow up questions based on a user response. However, as discussed in Section~\ref{sec:risks}, designers have to be careful about the potential harm this might cause to this vulnerable population group.
% Do we aspire to realize, for example, professional, casual, therapeutic or wholly transactional relationships, and which design choices realize these modes not only of interacting, but relating?
% \subsubsection{Imbuing Conversational Characteristics to Support Empathy}
% Consistent with the prior findings, our results show that participants valued \acl{CA} as a `good listener'~\cite{bauer2010introducing, clark2019makes}. Many participants in this study liked the fact that \acl{app} did not provide any feedback related to their self-report and expressed that such passive nature of the \ac{CA} served as a blank slate enabling them to vent their emotions. Others however, appreciated \acl{app}'s response feedback and expressed that it gave them a sense of being heard, although that was not the intention of the design as described in Table~\ref{tab:diary_mgmt}. Participants' desire for emotional support from the \ac{CA} suggests the need for designing agents with ability to formulate empathetic responses to user utterances to improve engagement. With the possibility of tailoring voice features such as tone, intonation, speed and pitch, speech-enabled \ac{CA}s could emulate an empathetic self-reporting experience. Related work in text-based \ac{CA} demonstrated that such interactions could have positive effects on their mental health and wellbeing~\cite{inkster2018empathy}. 
% \subsubsection{Supporting User Agency in Terms of Health Recommendations}
% As discussed in Section~\ref{sec:ca_ad}, \ac{CA}s are considered as an effective medium of employing methods of traditional therapies (e.g., \ac{CBT} and \ac{BA}) which involves providing recommendations to encourage positive health behavior change. Many participants in this study expressed that they would only consider non-intrusive recommendations provided with the opportunity to make their own decisions. Such attitude towards \ac{CA} indicates the need for designing systems that respect the sensitivity of their mental state in this vulnerable population group.
% \subsection{Enablers and Barriers to User Engagement}
% Engagement is a prominent goal for the design of many systems within \ac{HCI}, and has been touted as one of the potential advantages of speech-enabled systems. One way to support design for engagement in the practice of self-report via \ac{CA} is therefore to begin to understand its enablers and barriers.
% Enabler:convenience
% Many participants of this study demonstrated high levels of engagement (See Sections \ref{sec:participant_engagement},~\ref{sec:perceived_experience}), which often reflected broad acceptance of the technology as well as positive perceptions of the \ac{CA}'s potential to support mental health and wellbeing. 
% As motivations for their engagement in self-report via an agent, many participants, in line with prior findings~\cite{lau2018alexa, Rafal2018Workplace}, pointed to the convenience offered by the \ac{CA}'s hands-free experience; \textit{``I love it because it’s like an interface you talk directly to. It’s super easy to use. You don’t have to open your laptop and go to a specific page. I can just go home, open the door and talk to \acl{app}, super easy''} [P1]. 
% Enabler:natural interaction
% Speech, as a more natural form of interaction, was also described as allowing users to express their emotions more freely and spontaneously compared to other means of self-report; \textit{``Speaking is much easier because you can just let the words flow and you don't have to think about it''} [P14].
% Still other participants spoke to feeling heard -- a quality that we believe is fundamental for sustainable long term \ac{CA}-user relationships. Users' appropriation of the technology into their lives and for their own purposes therefore reflects diverse forms of value associated with the technology. P14 and P6, for example, even took the smart speaker with them when away from home; \textit{``So for three weeks I was in the sort of caravan and actually it worked fine. I just had to reset it to my, to the Wi-Fi, and that was not a problem''}[P14].       
% Barriers:technical limitations which prevented users from completing their self-reports, social factors, and privacy concerns including eavesdropping and data security
% And yet, as barriers to their engagement, participants often mentioned technical limitations which prevented users from completing their self-reports, social factors, and privacy concerns including eavesdropping and data security. Our findings therefore reflect participants' willingness to engage with \acp{CA} for the self-report of mental health and wellbeing, but also raise questions about the readiness of \acp{CA} for real-world use.
% When thinking about engagement as a design goal it is therefore important to resist a narrow focus on engagement as an aim in itself, and to think about how the technology best supports participants' own health and wellbeing goals, and how users are best able to appropriate the technology into their own lives. This is particularly important for users living with \ac{AD} and other vulnerable populations.
% \textbf{Our findings reveal users' experience as strongly shaped by actions taken to overcome the technological limitations of \acp{CA}, diverse personified perceptions of a self-report agent, the socially-contingent nature of self-reporting practice as well as users' reflections on privacy and security concerns.}
% \subsubsection{Emulate emotional support}
% \subsubsection{Establish positive tone or relationship}
% Privacy perception and data security concerns presented in this study show users' trust towards the governing bodies, yet their confidence over smart speaker companies remain skeptical. 
% Based on these results, we discuss: a, b, c 
% with respect to \ac{CA}'s potential in empowering users to self-report their mental health and wellbeing.
% We then, highlight several important design considerations for future design of \ac{CA} to support self-report of mental health and wellbeing.
% Notably, this result also reflects the gulf between the technology and the users expectation of being able to have a human-human-like conversation~\cite{Gulfbetwee2016ewa} despite setting a realistic expectations before \ac{CA} use. This leads 
% the question whether the technology is capable of engaging users for a desired outcome.
% participants with lower than 60\% of adherence rate (P1, P11, P20) reasoned that they were unable to use \ac{CA} due to the immobility of the smartspeaker, their own mental state and privacy reasons. 
% % immobility of the smartspeaker, 
% \textit{``When I am quite down, I close down. I don’t talk to anyone. It’s not just \acl{app}. During the times I was lived with my boyfriend, it was difficult for me to ask him to leave the room as I wanted privacy to talk to \acl{app}. And sometime, I wasn’t in town''}.
% Enablers
% 1. There are so many technical limitations of \ac{CA} but people are ready and willing to use the system - reflected by our results:
% UEQ 
% Adherence
% tactics they used and 
% the social and mental benifits they outlined.
% Enablers of Engagement
% Despite many technical challenges, results 
% Will \ac{CA} be able to relate 
% P14 in particular questioned a \ac{CA}'s ability to relate users' emotions to their past and commented as a result that \acp{CA} might not be able to establish truly effective patient-therapist (\ac{CA} or human) relationships while acknowledging that talking to \acp{CA} might help one to reflect on their emotions.
% \textit{``You can have self reflection and you can figure out things by yourself. But sometimes it is also important that someone else can say `okay, you're feeling this way but maybe it comes from this' and they can relate things that have been mentioned in other conversations or those relationships cannot be made with \acl{app} so and that's why I think it's more useful to have sometimes meetings with the psychiatrists.''} [P14]
% Reasons for Low Rates of
% Participants with lower than 60\% of adherence rate (P1, P11, P20) reasoned that they were unable to use \ac{CA} due to
% the immobility of the smartspeaker, 
% their own mental state and 
% privacy reasons. 
% immobility of the smartspeaker, 
% P1, for example, when asked about her low adherence rate, they explained: 
% \textit{``When I am quite down, I close down. I don’t talk to anyone. It’s not just \acl{app}. During the times I was lived with my boyfriend, it was difficult for me to ask him to leave the room as I wanted privacy to talk to \acl{app}. And sometime, I wasn’t in town''}.
% Despite the static nature of the smart speaker, P14 and P6 however, took the smart speaker with them when they were away from their home and described that it was not much of a hassle to set up the system in a new place as long as they had access to the electric outlet and Wi-fi.
% \begin{quote}
% \textit{``So for three weeks I was in the sort of caravan and actually it worked fine. I just had to reset it to my, to the Wi-Fi, and that was not a problem.''}
% [P14]
% r27
% \end{quote}    
% Participants shared many valuable suggestions for the future design of a \acp{CA} to support self-reports of mental health and wellbeing. Their suggestions included ways to 
% (i) improve \ac{CA}'s conversational skill;
% (ii) reflect on self-reports; and provide
% (iii) health recommendations.
% ``past is just a story we tell to ourselves''~\cite{spike2014her}
% Who are they reporting to in participant's view?
% Who is the conversation with?
% Where and how does `conversation' add nuance?
% P7 shared similar thoughts and added that appropriate probing would make the conversation more natural and emulate the sense of being heard.
%     \textit{``I would like some maybe maybe basic minimal but some sort of a simulation of listening you know, like she like she would pick up on the small things in the conversation and ask an ask questions based on that. Of course that's the hardest part you know, making them think Yeah, you know, like the way the natural people communicate you know, like if someone if someone said you know, `Man my boss really frustrates me', you know, then \acl{app} could ask him, `Oh, what is so frustrating about her?' or `What did she say?' and stuff like that. That sort of things will make the conversation more organic''}.
% ----sense of confrontation----
% ---- \ac{CA} doesn't make them feel like shit ----
% HOW to present the data for reflection?
% WHY NOT Vocal reflection?
% It is therefore important to ask the question as to whether \ac{CA} technology is ready for such use cases. 
% Suggesting a supplemental mobile app to reflect on their data, P7 said, 
% \textit{``If \acl{app} was telling me my statistics, I would probably space out through half of it. And I would be like, yeah, is there an option to repeat this? So a supplemental app would be great''}.
% Assuming that \acl{CA} would play the recordings of their self-reports to reflect on P5 noted that they did not like to listen to themselves and the graphical would be more appropriate for reflection:
% \textit{``I don’t like to listen to myself. Reading could be better. You can search easily with your eyes and go through the words - find interesting parts''}.
% Participants shared a common view of the need for a visual tool to reflect on their data although many also entertained the idea of verbal reflection. While several participants expressed their interest in verbal reflection, some also believed that it could serve as an effective tool for sustainable behavior change.
% This included (i) making multiple entries, (ii) adapting one's speech, and (iii) preparing in advance to self-report. A
% Many of participants comments also reflected their own respect for the thoughts of their social circle.
% Narratives of the participants' social circle strengthens \ac{CA}'s social acceptance and its potential in supporting users' relationship with their significant others, albeit, .
% Awareness of this position raises questions concerning the ethics of various framings of the technology. Many researches are working on personified interpretations of agents [cite work on penguins etc], and in this study many participants assigned human-like qualities to \acl{app}. And yet 
% Participants suggested several other suggestion to support engaging self-report via \ac{CA}. These design suggestions included 
% (i) notification, 
% (ii) humor, and
% (iii) health information.
% notification for adherence
% P1 and P16 suggested notifying users to talk to \ac{CA} if they have not self-reported for certain period of time.
% \textit{``When I am down, I close myself to the world, it would be an important moment for me to actually speak. Therefore, if I have not been speaking for lets say 48 hrs, \acl{app} could notify me or ask me, `Hey, are you feeling okay?' ''} [P1].
% humor
% Echoing the prior literature~\cite{clark2019makes}, P4 remarked humor in the conversation could make them feel positive:
% \textit{``If you can get people to smile maybe they'll be more positive about their day...say something funny''}.
% information on the issue	
% P4 also mentioned that providing relevant health information (e.g., symptoms of a health condition) could make feel that they are not the alone:
% \textit{``It could read you something from the web page, like `(name of the site) says this about anxiety or blah blah blah'. That makes you kind of like, Okay other people feel these things also when they have anxiety'' }.
% \subsection{Users Are Ready to Engage. How about \acp{CA}?} % Is \ac{CA} technology ready?
% Engagement is a prominent goal for the design of many systems within \ac{HCI}, and has been touted as one of the potential advantages of speech-enabled systems. How then might we design for engagement in the practice of self-report via \ac{CA}?
% subsubsection{To use a \ac{CA} might be convenient but having conversation with it is a different story.} 
% Its is convenient 
% Many participants of this study demonstrated high levels of engagement (See Sections \ref{sec:perceived_engagement} \& ~\ref{sec:perceived_experience}) which often reflected broad acceptance of the technology as well as positive perceptions of the potential of \acp{CA} to support mental health and wellbeing. As motivations for their engagement in self-report via an agent, many participants, in line with prior findings~\cite{lau2018alexa, Rafal2018Workplace}, pointed to the convenience offered by the \ac{CA}'s hands-free experience; \textit{``I love it because it’s like an interface you talk directly to. It’s super easy to use. You don’t have to open your laptop and go to a specific page. I can just go home, open the door and talk to \acl{app}, super easy''} [P1]. 
% But doesn't have the fundamental conversational feature like pause 
% More research on designing c?
% \subsubsection{Being heard is important for this group of population but how can we design for it?} 
% how can CA do that? Is it capable?
% Speech as a more natural form of interaction, allows users to express their emotions freely and spontaneously compared to other means of self-report; \textit{``Speaking is much easier because you can just let the words flow and you don't have to think about it''} [P14]. Still others spoke to feeling heard -- a quality that we believe is fundamental for sustainable long term \ac{CA}-user relationships. And yet, as barriers to their engagement, participants often mentioned technical limitations which prevented users from completing their self-reports, social factors, and privacy concerns including eavesdropping and data security. Our findings therefore reflect participants' willingness to engage with \acp{CA} for the self-report of mental health and wellbeing, but also raise questions about the readiness of \acp{CA} for real-world use. 
% \subsubsection{Does CA has to be that sophisticated? What do users want?} 
% Technical limitations (See Section~\ref{sec:ca_limitations}) undoubtedly restrained users from utilizing the \ac{CA} to its full potential, a result that aligns with prior research findings~\cite{Rafal2018Workplace}. Interestingly many of the limitations which most challenged participants are not strictly technical in nature but due to artificial constraints imposed on the interaction design itself. This highlights the need for a future human-centered approach to \ac{CA} design and broader consideration of a wider variety of use-cases for such systems. 
% participants were 
% \begin{quote}
% \vspace{2mm}
% \textit{``If \acl{app} can do, not to automate the life, to be someone to talk to. Yes, I can do that everyday. I can trust to talk to \acl{app} everyday. I don't know if you have ever seen the movie, `Her'~\cite{spike2014her}. Something like that. The `Her' thing is not to automate your life or to remind you anything. Google Assistant can do that. Someone to talk with, not to talk with to get your feedback, just to talk with to review your day and to see okay what you're actually doing.''} [P16]
% \vspace{2mm}
% \end{quote} 
% Looking forward, as smart-speaker devices become more mainstream, it is worth considering the futures we desire, and how we might design to sustainably support them. What role do we want these devices to play in our lives? 
% Although most participants expressed hesitation in considering health recommendations provided by a \ac{CA}, several mentioned that they would welcome non-intrusive recommendations that reminded them of the things they could do, although added that a \ac{CA} should allow them to make their own decisions to act;  \textit{``I mean sometimes you forget. So you can remind sometimes. It should be not like `Okay, here you have a breathing exercise'. Like `Do you want to do a breathing exercise to improve your mental health?' and then you can say `Yes' or `No'. I think it's important that you still have the possibility to say no''} [P14]. Other participants suggested incorporating humour and advice into conversations with \acl{app}.
% These possibilities raise additional questions. Do we, for example, want agents to provide recommendations for our health and wellbeing, or prove truly capable of holding rich conversations, or might we prefer uncaring machines, who may also ironically be best placed to provide an experience of care? These are the kinds of questions raised by this study, which is one of the first to engage a vulnerable group in the real-world use of such technologies, despite the fact that many such systems are available and continue to emerge on the commercial app stores. It is time we begin to reflect on the implications of growing adoption of these technologies, and we hope this work serves as a step in the direction of such a discourse.      
% This study therefore reveals the extent to which, when designing for engagement, it is also important to consider users', designers' and researchers' implicit and explicit motivations. 
% Is there such a thing as `the uncanny value of caring'? Where does it start and where does it end?
% The phrase `designing for conversation' may therefore itself be read as a re-framing of the very idea of self-report.
% Indeed, even more broadly, which characteristics of human-to-human relationships translate to human-to-agent relationships?
% In part, participants' varied interpretations of the agent may be explained by our own open-ended presentation of the system -- a conscious choice. We often struggled to provide a concise description of our own framing of this agent, something we have noticed much nascent work in this space struggles to achieve, as researchers and designers strive to strike the appropriate balance between human and machine. To overcome such barriers to action and clarify our own expression, it can be useful to ask ourselves such questions as `From the participant's point of view, who, or what, are they reporting to?' Who is the conversation with? And where does conversation add value? Making these kinds of conceptions clear to participants is likely key to engendering trust.
% \ac{VUI} technologies are increasingly presented as capable of supporting more human modes of interaction, and in turn characterized and analyzed in terms of interactional traits, from responsiveness to tone and ease of understanding. We argue that designers of \ac{VUI} systems must also consider how particular relationship framings interact with the limitations and possibilities of technology. A more professional framing for example, can enable designers to side-step certain limitations of the technology by, from the offset, setting expectations which preclude more casual and intimate forms of interaction. We must then ask, which technological futures do we desire?
% While many participants in this study found these strategies feasible, the requirement of these additional actions to appropriate the technology undoubtedly added burden on the users. Research shows that such burden could could have negative impact on user experience, adherence as well as the quality of the self-reports~\cite{doherty2020design}.